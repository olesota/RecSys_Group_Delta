{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## item-similarity\nclass collab_filter():\n    def __init__(self):\n        self.train_data = None\n        self.user_id = None\n        self.item_id = None\n        #self.co_matrix = None\n    \n    ## retrieve all unique items for all users\n    def get_user_items(self):\n        user_items = self.train_data.groupby(self.user_id)[self.item_id].apply(lambda g: np.unique(g.values)\\\n                                                                                         .tolist()).to_dict()\n        return user_items\n    \n    ## retrieve all unique users for all items\n    def get_item_users(self):\n        item_users = self.train_data.groupby(self.item_id)[self.user_id].apply(lambda g: np.unique(g.values)\\\n                                                                                         .tolist()).to_dict()\n        return item_users\n    \n    ## filter user_items co-occurence for each unique item\n    def filter_items(self, user_items, item_users, keys):\n        filtered = {k: [user_items[x] for x in item_users[k]] for k in keys}\n        \n        return filtered\n    ## flatten item-user_items co-occurence \n    def flat_clear(self, filtered):\n        flattened = { k: list(set(itertools.chain(*v))) for k, v in filtered.items() }\n        ## remove matching keys and items from co-occurrence \n        cleared = { k: [v for v in flattened[k] if v != k] for k in flattened.keys() }\n        \n        return cleared\n    \n    ## co-occurrence matrix\n    def co_occurrence_mat(self, item_users, cleared):\n        item_users_set = { k: set(v) for k, v in item_users.items() }\n\n        co_occurrence = { k: [(v, len(item_users_set[k].intersection(item_users_set[v])), len(item_users_set[k]\\\n                                    .union(item_users_set[v]))) for v in cleared[k]] for k in cleared.keys()}\n        \n        ## normalization using ratio of intersection over union(Jaccard similarity)\n        normalized_co_occurrence = { k: [(v[0], v[1]/float(v[2])) for v in co_occurrence[k]]\\\n                                                              for k in co_occurrence.keys() }\n        \n        return normalized_co_occurrence\n    ## compute similarity score\n    def compute_sim_score(self, user_items, user, co_occurrence):\n        l = len(user_items[user])\n        return { k: sum([v[1] for v in co_occurrence[k]]) / l for k in co_occurrence.keys()}\n    \n    ## use co-occurrence matrix\n    def generate_recommendations(self, user, co_occurr_mat, user_items):\n        ## calculate weighted average of score for each user_items\n        sim_score = self.compute_sim_score(user_items, user, co_occurr_mat)\n        \n        ## sort in descending order of sim_score\n        sort_sim_score = [(k, sim_score[k]) for k in sorted(sim_score, key=sim_score.get, reverse=True)]\n        columns = ['user_id', 'item_id', 'score', 'rank']\n        df = pd.DataFrame(columns=columns)\n        \n        ## fill with top 25 item based recommendations\n        rank =1\n        for i in range(0, len(sort_sim_score)):\n            if rank <= 25:\n                df.loc[len(df)] = [user, sort_sim_score[i][0], sort_sim_score[i][1],rank]\n                rank = rank+1\n        \n        df_out = df.groupby(['user_id']).apply(lambda df: ' '.join(df['item_id'].tolist()))\\\n                                               .reset_index(name='item_recommendations')\n        return df_out\n        \n    ## create model\n    def create(self, train_data, user_id, item_id):\n        self.train_data = train_data\n        self.user_id = user_id\n        self.item_id = item_id\n    \n    def recommend(self, user):\n        user_items = self.get_user_items()\n        item_users = self.get_item_users()\n        keys = list(item_users.keys())\n        filtered = self.filter_items(user_items, item_users, keys)\n        cleared = self.flat_clear(filtered)\n        \n        ## co-occurrence matrix\n        co_occurr_matrix = self.co_occurrence_mat(item_users, cleared)\n        df_recommendations = self.generate_recommendations(user, co_occurr_matrix, user_items)\n \n        return df_recommendations","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data = pd.read_csv('../input/traindata/train.csv', delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/testdata/test.csv', delimiter=',')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## filter test data by action type\nmask = test_data[\"action_type\"] == \"clickout item\"\ntest_click = test_data[mask]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## filter train data by action type\n#mask2 = train_data[\"action_type\"] == \"clickout item\"\n#train_click = train_data[mask2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = train_click.copy()\n#df_train = df_train.iloc[:, [0,5]]\n#df_train = df_train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## drop NaN rows\ndf_test = test_click.copy()\ndf_test = df_test.iloc[:, [0,5]]\ndf_test = df_test.dropna()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##\ndf_user = df_test.copy()\ndf_user.rename(columns={'reference':'item_id'}, inplace=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generate recommendation for the test set\ndef generate_recommendations(df_user):\n    unique_users = list(np.unique(df_user['user_id'].values))\n    \n    sim_model = collab_filter()\n    sim_model.create(df_user, 'user_id', 'item_id')\n    \n    ## generate item recommendations for each user_id\n    with open('submission_collab_filter.csv', 'a') as f:\n        f.write('user_id' + ',' + 'item_recommendations\\n')\n        recommends = []\n        for user in unique_users[50000:]:\n            rec = sim_model.recommend(user)\n            f.write(str(rec['user_id'].values[0]) + ',' + str(rec['item_recommendations'].values[0]) + '\\n')\n            recommends.append(rec)    \n    df = pd.concat(recommends, sort=False)\n    \n    return df","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = generate_recommendations(df_user)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}